{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJMoX0VotLAuBJx/0EixT+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YunshuoTian/nlp/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZug6vX-FzJY"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3M1gqvx1EpN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a dataset from hugginface\n",
        "dataset = load_dataset(\"akoksal/LongForm\")"
      ],
      "metadata": {
        "id": "ADTkb8fRI69D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how the data look like\n",
        "dataset['train'][101]"
      ],
      "metadata": {
        "id": "YAUx9HDUI3vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load tokenizer and model\n",
        "model_name = 'bert-base-uncased'\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "OD2z4uMZIzdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.vocab_size)\n",
        "print(tokenizer.model_max_length)"
      ],
      "metadata": {
        "id": "T7ihk0aeD0VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start preprocessing\n",
        "def preprocess_data(examples):\n",
        "    # Tokenize the inputs (e.g., context or question)\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples['input'],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_offsets_mapping=True,\n",
        "    )\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(tokenized_examples['offset_mapping']):\n",
        "        input_ids = tokenized_examples['input_ids'][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Find the answer in the input text\n",
        "        input_text = examples['input'][i]\n",
        "        answer_text = examples['output'][i]\n",
        "\n",
        "        start_char = input_text.find(answer_text)\n",
        "        if start_char == -1:  # Answer not found in the input\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "            continue\n",
        "\n",
        "        end_char = start_char + len(answer_text)\n",
        "\n",
        "        # Find the start and end token indices\n",
        "        token_start_index = 0\n",
        "        token_end_index = 0\n",
        "\n",
        "        for idx, (start, end) in enumerate(offsets):\n",
        "            if start <= start_char < end:\n",
        "                token_start_index = idx\n",
        "            if start < end_char <= end:\n",
        "                token_end_index = idx\n",
        "                break\n",
        "\n",
        "        start_positions.append(token_start_index)\n",
        "        end_positions.append(token_end_index)\n",
        "\n",
        "    tokenized_examples['start_positions'] = start_positions\n",
        "    tokenized_examples['end_positions'] = end_positions\n",
        "\n",
        "    # Remove the offset mapping since we don't need it anymore\n",
        "    tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "eLhhnrBREIeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the first 1000 data in dataset for training\n",
        "\n",
        "train_dataset = dataset[\"train\"].select(range(1000))\n",
        "eval_dataset = dataset[\"validation\"]\n",
        "tokenized_trainset = train_dataset.map(preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "tokenized_evalset = eval_dataset.map(preprocess_data, batched=True, remove_columns=dataset[\"validation\"].column_names)"
      ],
      "metadata": {
        "id": "XVYCGMTJELFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define training args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        ")"
      ],
      "metadata": {
        "id": "fNvegQm5EM8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator"
      ],
      "metadata": {
        "id": "_Iflix6IEOkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_trainset,\n",
        "    eval_dataset=tokenized_evalset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "OK6bHpvDERDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "sI-WqMkYESce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "_vgK6IylETr6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}